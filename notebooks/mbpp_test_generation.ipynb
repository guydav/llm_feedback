{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import sys\n",
    "import typing\n",
    "\n",
    "\n",
    "from IPython.display import display, Markdown, HTML\n",
    "\n",
    "import datasets\n",
    "from evaluate import load\n",
    "\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.chains import LLMChain, SequentialChain\n",
    "from langchain.prompts.chat import (\n",
    "    ChatPromptTemplate,\n",
    "    AIMessagePromptTemplate,\n",
    "    HumanMessagePromptTemplate,\n",
    ")\n",
    "from langchain.schema import (\n",
    "    SystemMessage\n",
    ")\n",
    "\n",
    "sys.path.append(os.path.abspath('..'))\n",
    "\n",
    "import llm_feedback.utils.env as env\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No config specified, defaulting to: mbpp/full\n",
      "Found cached dataset mbpp (/Users/guydavidson/.cache/huggingface/datasets/mbpp/full/1.0.2/4458a31cd4305553c8e88e3f0bfb94fc74fe1a9faeeb8c32ed166939735eaeff)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4c141e8e4ea84c7aab04ac4eaeae3881",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "os.environ[\"HF_ALLOW_CODE_EVAL\"] = '1'\n",
    "env.load_dotenv()\n",
    "mbpp_dataset = datasets.load_dataset('mbpp')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'task_id': 601,\n",
       " 'text': 'Write a function to find the longest chain which can be formed from the given set of pairs.',\n",
       " 'code': 'class Pair(object): \\r\\n\\tdef __init__(self, a, b): \\r\\n\\t\\tself.a = a \\r\\n\\t\\tself.b = b \\r\\ndef max_chain_length(arr, n): \\r\\n\\tmax = 0\\r\\n\\tmcl = [1 for i in range(n)] \\r\\n\\tfor i in range(1, n): \\r\\n\\t\\tfor j in range(0, i): \\r\\n\\t\\t\\tif (arr[i].a > arr[j].b and\\r\\n\\t\\t\\t\\tmcl[i] < mcl[j] + 1): \\r\\n\\t\\t\\t\\tmcl[i] = mcl[j] + 1\\r\\n\\tfor i in range(n): \\r\\n\\t\\tif (max < mcl[i]): \\r\\n\\t\\t\\tmax = mcl[i] \\r\\n\\treturn max',\n",
       " 'test_list': ['assert max_chain_length([Pair(5, 24), Pair(15, 25),Pair(27, 40), Pair(50, 60)], 4) == 3',\n",
       "  'assert max_chain_length([Pair(1, 2), Pair(3, 4),Pair(5, 6), Pair(7, 8)], 4) == 4',\n",
       "  'assert max_chain_length([Pair(19, 10), Pair(11, 12),Pair(13, 14), Pair(15, 16), Pair(31, 54)], 5) == 5'],\n",
       " 'test_setup_code': '',\n",
       " 'challenge_test_list': []}"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mbpp_dataset['train'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_gen_prompt_template = ChatPromptTemplate.from_messages([\n",
    "    SystemMessage(content=\"You are a helpful Python test generator, capable of generating tests that are helpful and evaluate code coverage and edge cases.\"),\n",
    "    HumanMessagePromptTemplate.from_template(\"\"\"\n",
    "You will be given a Python programming task and 3 unit tests. Please write 3 additional unit tests to help the developer test their code. The tests should be as helpful as possible, and should cover as many edge cases as possible. The tests should be written in the same style as the two provided tests. \n",
    "Imporant: Do not include the existing test cases in your solution! Output just the new test cases. Ensure that you use the same function and class names as in the existing tests. Each test case should be ready to copy-paste Python console and run.\n",
    "Instruction:\n",
    "\"{text}\"\n",
    "Current unit tests:\n",
    "```python \n",
    "{test_list_0}\n",
    "{test_list_1}\n",
    "{test_list_2}\n",
    "```\n",
    "New unit tests:\n",
    "```python\n",
    "\"\"\".strip(), input_variables=[\"text\", \"test_list_0\", \"test_list_1\", \"test_list_2\"]),\n",
    "])\n",
    "\n",
    "llm = ChatOpenAI(model_name=\"gpt-3.5-turbo-0613\")\n",
    "\n",
    "chain = LLMChain(llm=llm, prompt=test_gen_prompt_template)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [],
   "source": [
    "code_eval = load(\"guydav/restrictedpython_code_eval\")\n",
    "markdown_pattern = re.compile(r\"```\\w*\")\n",
    "\n",
    "DEFAULT_ADDITIONAL_GLOBALS = {\n",
    "    'all': all,\n",
    "    'dict': dict,\n",
    "    'filter': filter,\n",
    "    'map': map,\n",
    "    'max': max,\n",
    "    'sum': sum,\n",
    "    'enumerate': enumerate, \n",
    "    'reversed': reversed,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def _parse_test_cases(text: str):\n",
    "    current_start = -1\n",
    "\n",
    "    test_cases = []\n",
    "\n",
    "    while True:\n",
    "        current_start = text.find('assert', current_start + 1)\n",
    "        if current_start == -1:\n",
    "            break\n",
    "\n",
    "        current_end = text.find('\\n', current_start)\n",
    "        test_cases.append(text[current_start:current_end])\n",
    "\n",
    "    return test_cases\n",
    "\n",
    "def run_and_evaluate_chain(chain: LLMChain, mbpp_example: typing.Dict[str, typing.Any]):\n",
    "    out = chain({\n",
    "        \"text\": mbpp_example[\"text\"],\n",
    "        # HumanMessagePromptTemplate appears to not be able to handle lists,\n",
    "        # so we need to pass each element separately.\n",
    "        \"test_list_0\": mbpp_example[\"test_list\"][0],\n",
    "        \"test_list_1\": mbpp_example[\"test_list\"][1],\n",
    "        \"test_list_2\": mbpp_example[\"test_list\"][2],\n",
    "    })\n",
    "    \n",
    "    test_cases = _parse_test_cases(out[\"text\"])\n",
    "\n",
    "    solutions = [mbpp_example['code']]\n",
    "    solutions = [markdown_pattern.sub('', solution).strip() for solution in solutions]\n",
    "    solutions = [solution.replace('(object)', '') for solution in solutions]\n",
    "\n",
    "    result = code_eval.compute(\n",
    "        references=test_cases, \n",
    "        predictions=[solutions] * len(test_cases), \n",
    "        k=[len(solutions)],\n",
    "        allowed_imports=['typing', 'collections', 'math', 're', 'heapq'],\n",
    "        additional_globals=DEFAULT_ADDITIONAL_GLOBALS,\n",
    "        allow_str_format=True,\n",
    "        allow_underscore_variable_names=True,\n",
    "        timeout=60,\n",
    "        )[1]  # type: ignore\n",
    "    \n",
    "    return out, test_cases, solutions, result\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Notes\n",
    "* Frame around code coverage -- LOC hit, edge and corner cases, etc.\n",
    "    * Increase against both the existing unit tests, and randomly generated inputs\n",
    "* Provide the test-generating/feedback model with model-generated code, and evaluate the test cases generated on the model's code and the gold code\n",
    "* How do we know if this works? \n",
    "    * (1) tests pass on gold code\n",
    "    * (2) tests help identify cases where model-generated code passes the MBPP tests but misses something\n",
    "    * (3) compared to a random baseline [generate random inputs, query gold code for correct outputs, test against model-generated code]\n",
    "* (Potential thought for future: if we do a multi-turn thing, where the test-generating model gets a chance to revise the the tests based on the code's output, how psychophantic is the model -- does it tend to revise its tests to match the code's output, or does it maintain its original tests against this feedback -- type I vs type II errors, etc.)\n",
    "* Multi-turn dialog to fix errors or revise tests according to the code's output\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "defaultdict(list,\n",
       "            {1: [(0,\n",
       "               {'task_id': 1,\n",
       "                'passed': True,\n",
       "                'result': 'passed',\n",
       "                'completion_id': 0})],\n",
       "             2: [(0,\n",
       "               {'task_id': 2,\n",
       "                'passed': False,\n",
       "                'result': \"failed (<class 'AssertionError'>): \",\n",
       "                'completion_id': 0,\n",
       "                'exception_type': 'AssertionError',\n",
       "                'exception_description': ''})],\n",
       "             0: [(0,\n",
       "               {'task_id': 0,\n",
       "                'passed': True,\n",
       "                'result': 'passed',\n",
       "                'completion_id': 0})]})"
      ]
     },
     "execution_count": 164,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "index = 5\n",
    "out, test_cases, solutions, result = run_and_evaluate_chain(chain, mbpp_dataset['train'][index])\n",
    "\n",
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['assert radian_degree(0)==0',\n",
       " 'assert radian_degree(180)==3.141592653589793',\n",
       " 'assert radian_degree(270)==4.7123889803846']"
      ]
     },
     "execution_count": 166,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_cases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-1.1341838246359259e-07"
      ]
     },
     "execution_count": 157,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "np.radians(45.5) - 0.7941249230758024"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'task_id': 606,\n",
       " 'text': 'Write a function to convert degrees to radians.',\n",
       " 'code': 'import math\\r\\ndef radian_degree(degree):\\r\\n radian = degree*(math.pi/180)\\r\\n return radian',\n",
       " 'test_list': ['assert radian_degree(90)==1.5707963267948966',\n",
       "  'assert radian_degree(60)==1.0471975511965976',\n",
       "  'assert radian_degree(120)==2.0943951023931953'],\n",
       " 'test_setup_code': '',\n",
       " 'challenge_test_list': []}"
      ]
     },
     "execution_count": 167,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mbpp_dataset['train'][index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Write a function to check if the given integer is a prime number.\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "```python\n",
       "assert prime_num(13)==True\n",
       "```\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "```python\n",
       "assert prime_num(7)==True\n",
       "```\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "```python\n",
       "assert prime_num(-1010)==False\n",
       "```\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "```python\n",
       "def prime_num(num):\r\n",
       "  if num >=1:\r\n",
       "   for i in range(2, num//2):\r\n",
       "     if (num % i) == 0:\r\n",
       "                return False\r\n",
       "     else:\r\n",
       "                return True\r\n",
       "  else:\r\n",
       "          return False\n",
       "```"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "print(mbpp_dataset['train'][index]['text'])\n",
    "\n",
    "for test_case in mbpp_dataset['train'][index]['test_list']:\n",
    "    display(Markdown((f'```python\\n{test_case}\\n```\\n')))\n",
    "\n",
    "display(Markdown(f'```python\\n{(solutions[0])}\\n```'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "assert prime_num(2) == True\n",
      "assert prime_num(1) == False\n",
      "assert prime_num(0) == False\n",
      "assert prime_num(1000000007) == True\n"
     ]
    }
   ],
   "source": [
    "for c in test_cases:\n",
    "    print(c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "def prime_num(num):\n",
      "  if num >=1:\n",
      "   for i in range(2, num//2):\n",
      "     if (num % i) == 0:\n",
      "                return False\n",
      "     else:\n",
      "                return True\n",
      "  else:\n",
      "          return False\n"
     ]
    }
   ],
   "source": [
    "print(\"def prime_num(num):\\r\\n  if num >=1:\\r\\n   for i in range(2, num//2):\\r\\n     if (num % i) == 0:\\r\\n                return False\\r\\n     else:\\r\\n                return True\\r\\n  else:\\r\\n          return False\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n"
     ]
    }
   ],
   "source": [
    "def prime_num(num):\n",
    "\tif num >=1:\n",
    "\t\tfor i in range(2, num//2):\n",
    "\t\t\tif (num % i) == 0:\n",
    "\t\t\t\treturn False\n",
    "\t\t\telse:\n",
    "\t\t\t\treturn True\n",
    "\telse:\n",
    "\t\treturn False\n",
    "\n",
    "print(prime_num(15))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "defaultdict(list,\n",
       "            {0: [(0,\n",
       "               {'task_id': 0,\n",
       "                'passed': True,\n",
       "                'result': 'passed',\n",
       "                'completion_id': 0})],\n",
       "             1: [(0,\n",
       "               {'task_id': 1,\n",
       "                'passed': False,\n",
       "                'result': \"failed (<class 'AssertionError'>): \",\n",
       "                'completion_id': 0})],\n",
       "             2: [(0,\n",
       "               {'task_id': 2,\n",
       "                'passed': False,\n",
       "                'result': \"failed (<class 'IndexError'>): list index out of range\",\n",
       "                'completion_id': 0})],\n",
       "             3: [(0,\n",
       "               {'task_id': 3,\n",
       "                'passed': False,\n",
       "                'result': \"failed (<class 'AssertionError'>): \",\n",
       "                'completion_id': 0})]})"
      ]
     },
     "execution_count": 116,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tc = test_cases\n",
    "\n",
    "code_eval.compute(\n",
    "        references=tc, \n",
    "        predictions=[solutions] * len(tc), \n",
    "        k=[len(solutions)],\n",
    "        allowed_imports=['typing', 'collections', 'math', 're', 'heapq'],\n",
    "        additional_globals=DEFAULT_ADDITIONAL_GLOBALS,\n",
    "        allow_str_format=True,\n",
    "        allow_underscore_variable_names=True,\n",
    "        timeout=60,\n",
    "        )[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "sys.path.append('/Users/guydavidson/projects/restrictedpython_code_eval')\n",
    "import restrictedpython_code_eval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "metric = restrictedpython_code_eval.RestrictedPythonCodeEval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "defaultdict(list,\n",
       "            {0: [(0,\n",
       "               {'task_id': 0,\n",
       "                'passed': True,\n",
       "                'result': 'passed',\n",
       "                'completion_id': 0})]})"
      ]
     },
     "execution_count": 106,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "metric.compute(\n",
    "        references=tc, \n",
    "        predictions=[solutions] * len(tc), \n",
    "        k=[len(solutions)],\n",
    "        allowed_imports=['typing', 'collections', 'math', 're', 'heapq'],\n",
    "        additional_globals=DEFAULT_ADDITIONAL_GLOBALS,\n",
    "        allow_str_format=True,\n",
    "        allow_underscore_variable_names=True,\n",
    "        timeout=60,\n",
    "        )[1]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llm",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
