{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "import copy\n",
    "import itertools\n",
    "import os\n",
    "import sys\n",
    "\n",
    "from IPython.display import display, Markdown, HTML\n",
    "from Levenshtein import distance as _edit_distance\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from scipy import stats\n",
    "\n",
    "sys.path.append(os.path.abspath('..'))\n",
    "\n",
    "import llm_feedback.pilot.tasks as tasks\n",
    "from llm_feedback.pilot.tasks import mbpp\n",
    "from llm_feedback.utils.io import read_json\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# MBPP_OUTPUT_PATH = '../outputs/gpt-3.5-turbo-0613__gpt-3.5-turbo-0613__gpt-3.5-turbo-0613__mbpp-test-gen__train__2023_07_23_outputs.jsonl'\n",
    "# MBPP_OUTPUT_PATH = '../outputs/gpt-3.5-turbo-0613__gpt-3.5-turbo-0613__gpt-3.5-turbo-0613__mbpp-test-gen__train__shuffle_test__2023_07_23_outputs.jsonl'\n",
    "MBPP_OUTPUT_PATH = '../outputs/gpt-3.5-turbo-0613__gpt-3.5-turbo-0613__gpt-3.5-turbo-0613__mbpp-test-gen__train__2023_07_24_outputs.jsonl'\n",
    "\n",
    "mbpp_outputs = read_json(MBPP_OUTPUT_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "initial_solution gold_test accuracy: 0.83\n",
      "initial_solution model_test accuracy: 0.7279762459762459\n",
      "gold_code gold_test accuracy: 1.0\n",
      "gold_code model_test accuracy: 0.6748413253413255\n",
      "refinement gold_test accuracy: 0.8266666666666667\n",
      "refinement model_test accuracy: 0.7228666611166611\n",
      "6.05 3.0639027399707057 3 21\n"
     ]
    }
   ],
   "source": [
    "# TEST_ID_KEYS = ['0', '1', '2']\n",
    "GOLD_TEST_KEYS = ('test_list_0', 'test_list_1', 'test_list_2')\n",
    "INITIAL_SOLUTION = 'initial_solution'\n",
    "GOLD_CODE = 'gold_code'\n",
    "REFINEMENT = 'refinement'\n",
    "COMPLETION_ID_KEY = 'completion_id'\n",
    "RESULT_FIELD = 'result'\n",
    "PASSED_FIELD = 'passed'\n",
    "GOLD_TEST = 'gold_test'\n",
    "MODEL_TEST = 'model_test'\n",
    "\n",
    "SOLUTION_TYPES = [INITIAL_SOLUTION, GOLD_CODE]\n",
    "if REFINEMENT in mbpp_outputs[0]:\n",
    "    SOLUTION_TYPES.append(REFINEMENT)    \n",
    "\n",
    "TEST_TYPES = [GOLD_TEST, MODEL_TEST]\n",
    "\n",
    "accuracy_by_problem = {solution: {test_type: [] for test_type in TEST_TYPES} for solution in SOLUTION_TYPES}\n",
    "all_problem_result_summaries = []\n",
    "num_model_tests_by_problem = []\n",
    "\n",
    "missing_names = defaultdict(set)\n",
    "exception_types = defaultdict(set)\n",
    "\n",
    "for problem_id, problem_results in enumerate(mbpp_outputs):\n",
    "    problem_results_summary = {solution: {test_type: [] for test_type in TEST_TYPES} for solution in SOLUTION_TYPES}\n",
    "    test_id_keys = [key for key in problem_results.keys() if key.isdigit()]\n",
    "    for test_id in sorted(test_id_keys):\n",
    "        test_results = problem_results[test_id]\n",
    "        for i, code_test_results in test_results:\n",
    "            code_type = SOLUTION_TYPES[code_test_results[COMPLETION_ID_KEY]]\n",
    "            test_type = GOLD_TEST if int(test_id) < 3 else MODEL_TEST\n",
    "            code_test_result = code_test_results[RESULT_FIELD]\n",
    "            if code_test_result is None:\n",
    "                print(f'Problem {problem_id}, test {test_id} has result None')\n",
    "            if 'is not defined' in code_test_result.lower():\n",
    "                # print(f'Problem {problem_id}, test {test_id} has defintion issues: {code_test_result}')\n",
    "                name_index = code_test_result.find('name')\n",
    "                if name_index != -1:\n",
    "                    space_index = code_test_result.find(' ', name_index)\n",
    "                    next_space_index = code_test_result.find(' ', space_index + 1)\n",
    "                    missing_name = code_test_result[space_index + 1:next_space_index]\n",
    "                    missing_names[missing_name.replace(\"'\", '')].add(problem_id)\n",
    "            if '<class' in code_test_result:\n",
    "                exception_class_index = code_test_result.find('<class')\n",
    "                exception_start = code_test_result.find(\"'\", exception_class_index)\n",
    "                exception_end = code_test_result.find(\"'\", exception_start + 1)\n",
    "                exception_type = code_test_result[exception_start + 1:exception_end]\n",
    "                exception_types[exception_type].add(problem_id)\n",
    "            code_test_passed = code_test_results[PASSED_FIELD]\n",
    "            if code_test_passed is None:\n",
    "                print(f'Problem {problem_id}, test {test_id} has passed None')\n",
    "                code_test_passed = False\n",
    "            problem_results_summary[code_type][test_type].append(code_test_passed)\n",
    "\n",
    "    all_problem_result_summaries.append(problem_results_summary)\n",
    "    for model_type in SOLUTION_TYPES:\n",
    "        for test_type in TEST_TYPES:\n",
    "            model_problem_results = problem_results_summary[model_type][test_type]\n",
    "            if model_type == INITIAL_SOLUTION and test_type == GOLD_TEST:\n",
    "                model_problem_results = model_problem_results[1:]\n",
    "            accuracy_by_problem[model_type][test_type].append(np.mean(model_problem_results))\n",
    "            \n",
    "\n",
    "for model_key, key_results in accuracy_by_problem.items():\n",
    "    for test_key, test_key_results in key_results.items():\n",
    "        print(f'{model_key} {test_key} accuracy: {np.mean(test_key_results)}')\n",
    "\n",
    "\n",
    "num_model_tests_by_problem = [len(t[INITIAL_SOLUTION][MODEL_TEST]) for t in all_problem_result_summaries]\n",
    "print(np.mean(num_model_tests_by_problem), np.std(num_model_tests_by_problem), np.min(num_model_tests_by_problem), np.max(num_model_tests_by_problem))\n",
    "\n",
    "# print('Problems with fewest model tests:')\n",
    "# for problem_id in np.argsort(num_model_tests_by_problem)[:10]:\n",
    "#     print(f'Problem {problem_id}: {num_model_tests_by_problem[problem_id]}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Problem 3, test 1 model test results differ: True, False\n",
      "Problem 8, test 2 model test results differ: False, True\n",
      "Problem 40, test 0 model test results differ: True, False\n",
      "Problem 40, test 1 model test results differ: True, False\n",
      "Problem 40, test 2 model test results differ: True, False\n",
      "Problem 44, test 0 model test results differ: True, False\n",
      "Problem 44, test 1 model test results differ: True, False\n",
      "Problem 44, test 2 model test results differ: True, False\n",
      "Problem 45, test 0 model test results differ: True, False\n",
      "Problem 45, test 1 model test results differ: True, False\n",
      "Problem 59, test 0 model test results differ: False, True\n",
      "Problem 59, test 2 model test results differ: False, True\n",
      "Problem 67, test 0 model test results differ: False, True\n",
      "Problem 67, test 1 model test results differ: False, True\n",
      "Problem 67, test 2 model test results differ: False, True\n",
      "On test type gold_test, refinement better: 6, worse: 9\n",
      "\n",
      "Problem 8, test 1 model test results differ: True, False\n",
      "Problem 8, test 2 model test results differ: True, False\n",
      "Problem 9, test 2 model test results differ: False, True\n",
      "Problem 10, test 1 model test results differ: False, True\n",
      "Problem 12, test 4 model test results differ: False, True\n",
      "Problem 24, test 5 model test results differ: False, True\n",
      "Problem 25, test 0 model test results differ: True, False\n",
      "Problem 25, test 1 model test results differ: True, False\n",
      "Problem 25, test 2 model test results differ: True, False\n",
      "Problem 27, test 0 model test results differ: True, False\n",
      "Problem 27, test 2 model test results differ: True, False\n",
      "Problem 27, test 3 model test results differ: True, False\n",
      "Problem 27, test 5 model test results differ: True, False\n",
      "Problem 27, test 7 model test results differ: True, False\n",
      "Problem 44, test 0 model test results differ: True, False\n",
      "Problem 44, test 1 model test results differ: True, False\n",
      "Problem 44, test 2 model test results differ: True, False\n",
      "Problem 45, test 0 model test results differ: True, False\n",
      "Problem 45, test 1 model test results differ: True, False\n",
      "Problem 45, test 2 model test results differ: True, False\n",
      "Problem 56, test 3 model test results differ: False, True\n",
      "Problem 58, test 6 model test results differ: False, True\n",
      "Problem 59, test 1 model test results differ: False, True\n",
      "Problem 59, test 3 model test results differ: False, True\n",
      "Problem 59, test 4 model test results differ: False, True\n",
      "Problem 62, test 2 model test results differ: False, True\n",
      "Problem 62, test 4 model test results differ: False, True\n",
      "Problem 64, test 0 model test results differ: False, True\n",
      "Problem 67, test 0 model test results differ: False, True\n",
      "Problem 71, test 1 model test results differ: False, True\n",
      "Problem 71, test 6 model test results differ: False, True\n",
      "Problem 72, test 0 model test results differ: False, True\n",
      "Problem 72, test 2 model test results differ: False, True\n",
      "Problem 72, test 8 model test results differ: False, True\n",
      "Problem 72, test 10 model test results differ: False, True\n",
      "Problem 78, test 2 model test results differ: False, True\n",
      "Problem 78, test 3 model test results differ: False, True\n",
      "Problem 78, test 4 model test results differ: True, False\n",
      "Problem 82, test 3 model test results differ: False, True\n",
      "Problem 83, test 8 model test results differ: False, True\n",
      "Problem 89, test 6 model test results differ: False, True\n",
      "Problem 91, test 3 model test results differ: False, True\n",
      "Problem 92, test 1 model test results differ: True, False\n",
      "On test type model_test, refinement better: 25, worse: 18\n",
      "\n",
      "Examples with change in both test types: {67, 8, 44, 45, 59}\n"
     ]
    }
   ],
   "source": [
    "examples_with_change = {}\n",
    "\n",
    "for test_type in TEST_TYPES:\n",
    "    refinement_better_count = 0\n",
    "    refinement_worse_count = 0\n",
    "    examples_with_change[test_type] = []\n",
    "\n",
    "    for i, rs in enumerate(all_problem_result_summaries):\n",
    "        for j in range(len(rs[INITIAL_SOLUTION][test_type])):\n",
    "            if rs[INITIAL_SOLUTION][test_type][j] != rs[REFINEMENT][test_type][j]:\n",
    "                examples_with_change[test_type].append(i)\n",
    "                print(f'Problem {i}, test {j} model test results differ: {rs[INITIAL_SOLUTION][test_type][j]}, {rs[REFINEMENT][test_type][j]}')\n",
    "                if rs[REFINEMENT][test_type][j]:\n",
    "                    refinement_better_count += 1\n",
    "                else:\n",
    "                    refinement_worse_count += 1\n",
    "\n",
    "    print(f'On test type {test_type}, refinement better: {refinement_better_count}, worse: {refinement_worse_count}\\n')\n",
    "\n",
    "print(f'Examples with change in both test types: {set(examples_with_change[GOLD_TEST]).intersection(set(examples_with_change[MODEL_TEST]))}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Matched-pairs t-tests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for test_type in TEST_TYPES:\n",
    "    for first_model_type, second_model_type in itertools.combinations(SOLUTION_TYPES, 2):\n",
    "        first_model_test_results = [rs[first_model_type][test_type] for rs in all_problem_result_summaries]\n",
    "        second_model_test_results = [rs[second_model_type][test_type] for rs in all_problem_result_summaries]\n",
    "        \n",
    "        first_model_test_results = np.array([t for tl in first_model_test_results for t in tl], dtype=float)\n",
    "        second_model_test_results = np.array([t for tl in second_model_test_results for t in tl], dtype=float)\n",
    "\n",
    "        result = stats.ttest_rel(first_model_test_results, second_model_test_results)\n",
    "\n",
    "        print(f'For {test_type} tests on {first_model_type} vs {second_model_type}: {result.statistic:.4f}, {result.pvalue:.4f}')\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "edit_distances = []\n",
    "\n",
    "for i, output in enumerate(mbpp_outputs):\n",
    "    model_tests = set(output['model_test_cases'])\n",
    "    gold_tests = set(output['test_cases'])\n",
    "    intersection = model_tests.intersection(gold_tests)\n",
    "    if intersection:\n",
    "        print(f'Found overlapping tests in #{i}: {intersection}')\n",
    "\n",
    "    output_edit_distances = [\n",
    "        _edit_distance(model_test, gold_test)\n",
    "        for model_test in model_tests\n",
    "        for gold_test in gold_tests\n",
    "    ]\n",
    "    edit_distances.append(output_edit_distances)\n",
    "\n",
    "\n",
    "min_edit_distances = [min(dists) for dists in edit_distances]\n",
    "mean_edit_distances = [np.mean(dists) for dists in edit_distances]\n",
    "\n",
    "print(f'Mean-min edit distance: {np.mean(min_edit_distances)}')\n",
    "print(f'Mean-mean edit distance: {np.mean(mean_edit_distances)}')\n",
    "\n",
    "sorted_indices = np.argsort(min_edit_distances)\n",
    "for i in range(10):\n",
    "    idx = sorted_indices[i]\n",
    "    print(f'#{i} ({idx}) min edit distance: {min_edit_distances[idx]}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_outputs(index):\n",
    "    output = mbpp_outputs[index]\n",
    "    display(Markdown(f'Problem text: {output[\"text\"]}'))\n",
    "    \n",
    "    code_block_lines = [\n",
    "        '```python', \n",
    "        '# Gold code:', \n",
    "        output['gold_code'], \n",
    "        '',\n",
    "        '# Model code:',\n",
    "        output['initial_solution'],\n",
    "        '',\n",
    "    ]\n",
    "\n",
    "    for i, test in enumerate(output['test_cases']): \n",
    "        code_block_lines.append(f'# Gold Test #{i} (Gold passed = {all_problem_result_summaries[index][GOLD_CODE][GOLD_TEST][i]}, Model passed = {all_problem_result_summaries[index][INITIAL_SOLUTION][GOLD_TEST][i]}):')\n",
    "        code_block_lines.append(test)\n",
    "\n",
    "    code_block_lines.append('')\n",
    "\n",
    "    for i, test in enumerate(output['model_test_cases']):\n",
    "        code_block_lines.append(f'# Model Test #{i} (Gold passed = {all_problem_result_summaries[index][GOLD_CODE][MODEL_TEST][i]}, Model passed = {all_problem_result_summaries[index][INITIAL_SOLUTION][MODEL_TEST][i]}):')\n",
    "        code_block_lines.append(test)\n",
    "\n",
    "    code_block_lines.append('```')\n",
    "    display(Markdown('\\n'.join(code_block_lines)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gold_code_model_test_accuracies = []\n",
    "\n",
    "\n",
    "for i, acc in enumerate(accuracy_by_problem[INITIAL_SOLUTION][GOLD_TEST]):\n",
    "    if acc != 1.0:\n",
    "        gold_code_model_test_acc = accuracy_by_problem[GOLD_CODE][MODEL_TEST][i]\n",
    "        print(i, acc,gold_code_model_test_acc )\n",
    "        gold_code_model_test_accuracies.append(gold_code_model_test_acc)\n",
    "\n",
    "plt.hist(gold_code_model_test_accuracies, bins=10)\n",
    "plt.title('(Gold Code | Model Tests) Accuracy where (Model Code | Gold Tests) Accuracy < 1.0')\n",
    "# accuracy_by_problem"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "[i for i, out in enumerate(mbpp_outputs) if 'prime' in out['text']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "visualize_outputs(84)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(model_solution_test_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, acc in enumerate(accuracy_by_problem[INITIAL_SOLUTION][MODEL_TEST]):\n",
    "    gold_code_model_test_acc = accuracy_by_problem[GOLD_CODE][MODEL_TEST][i]\n",
    "    if acc > gold_code_model_test_acc:\n",
    "        print(i, acc, gold_code_model_test_acc)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "visualize_outputs(48)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for out in mbpp_outputs:\n",
    "    if out['test_list_0'].count('==') != 1:\n",
    "        print(out['test_list_0'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tl = [mbpp_outputs[48]['test_list_0'], mbpp_outputs[48]['test_list_1'], mbpp_outputs[48]['test_list_2']]\n",
    "cases, expected_outputs = zip(*[t.split('==') for t in tl])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime\n",
    "datetime.datetime.now().strftime('%Y_%m_%d')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy_by_solution_type = {k: np.mean(v) for k, v in accuracy_by_problem.items()}\n",
    "accuracy_by_solution_type"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "full_accuracy_rate_by_solution_type = {k: np.mean(np.array(v) == 1.0) for k, v in accuracy_by_problem.items()}\n",
    "full_accuracy_rate_by_solution_type"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feedback_acuracy_change_by_problem = np.array([accuracy_by_problem[REFINEMENT][i] - accuracy_by_problem[INITIAL_SOLUTION][i] for i in range(len(accuracy_by_problem[REFINEMENT]))])\n",
    "feedback_change_indices = np.where(feedback_acuracy_change_by_problem != 0)[0]\n",
    "feedback_acuracy_change_by_problem[feedback_change_indices], feedback_change_indices, len(feedback_change_indices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, delta in enumerate(feedback_acuracy_change_by_problem):\n",
    "    if delta < 0:\n",
    "        print(i, mbpp_outputs[i]['feedback'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SyntaxError, ZeroDivisionError, TypeError, NotImplementedError\n",
    "# Indices to re-check: 34, 52, 57, 85\n",
    "exception_types['SyntaxError']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "i =57\n",
    "print(mbpp_outputs[i]['text'])\n",
    "print(mbpp_outputs[i]['initial_solution'])\n",
    "print(mbpp_outputs[i]['feedback'])\n",
    "print(mbpp_outputs[i]['refinement'])\n",
    "\n",
    "for test_id in TEST_ID_KEYS:\n",
    "    print(mbpp_outputs[i][f'test_list_{test_id}'])\n",
    "    results = [test_results[1]['result'] for test_results in mbpp_outputs[i][test_id]]\n",
    "    print(results)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import heapq\n",
    "\n",
    "def heap_sort(lst):\n",
    "    heap = []\n",
    "    for value in lst:\n",
    "        heapq.heappush(heap, value)\n",
    "    \n",
    "    ordered = []\n",
    "    while heap:\n",
    "        ordered.append(heapq.heappop(heap))\n",
    "    \n",
    "    return ordered\n",
    "\n",
    "assert heap_sort([1, 3, 5, 7, 9, 2, 4, 6, 8, 0])==[0, 1, 2, 3, 4, 5, 6, 7, 8, 9]\n",
    "assert heap_sort([25, 35, 22, 85, 14, 65, 75, 25, 58])==[14, 22, 25, 25, 35, 58, 65, 75, 85]\n",
    "assert heap_sort( [7, 1, 9, 5])==[1,5,7,9]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mbpp_outputs[i]['0']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist(feedback_acuracy_change_by_problem, bins=20)\n",
    "plt.show()\n",
    "plt.hist(feedback_acuracy_change_by_problem[feedback_change_indices])\n",
    "plt.title('Change in test pass rate after feedback')\n",
    "plt.ylabel('Count')\n",
    "plt.xlabel('Change in test pass rate')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from evaluate import load\n",
    "import re\n",
    "code_eval = load(\"guydav/restrictedpython_code_eval\")\n",
    "markdown_pattern = re.compile(r\"```\\w*\")\n",
    "\n",
    "import os\n",
    "os.environ[\"HF_ALLOW_CODE_EVAL\"] = '1'\n",
    "\n",
    "ALLOWED_IMPORTS = ['typing', 'collections', 'math', 're', 'heapq', 'itertools', 'sys']\n",
    "DEFAULT_ADDITIONAL_GLOBALS = {\n",
    "    'all': all,\n",
    "    'dict': dict,\n",
    "    'filter': filter,\n",
    "    'map': map,\n",
    "    'max': max,\n",
    "    'min': min,\n",
    "    'sum': sum,\n",
    "    'enumerate': enumerate,\n",
    "    'reversed': reversed,\n",
    "    'iter': iter,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# output = mbpp_outputs[95]\n",
    "output = copy.deepcopy(mbpp_outputs[52])\n",
    "\n",
    "model_test_cases = mbpp._parse_test_cases(output['feedback'])\n",
    "test_cases = [output[key] for key in ('test_list_0', 'test_list_1', 'test_list_2')] + model_test_cases\n",
    "solutions = [output['initial_solution'], output['gold_code']]\n",
    "solutions = [markdown_pattern.sub('', solution).strip() for solution in solutions]\n",
    "solutions = [solution.replace('(object)', '') for solution in solutions]\n",
    "if output['test_setup_code']:\n",
    "    solutions = ['\\n'.join([output['test_setup_code'], solution]) for solution in solutions]\n",
    "\n",
    "r = code_eval.compute(\n",
    "    references=test_cases, \n",
    "    predictions=[solutions] * len(test_cases), \n",
    "    k=[len(solutions)],\n",
    "    allowed_imports=ALLOWED_IMPORTS,\n",
    "    additional_globals=DEFAULT_ADDITIONAL_GLOBALS,\n",
    "    timeout=60,\n",
    "    allow_str_format=True,\n",
    "    allow_underscore_variable_names=True,\n",
    "    )[1]  # type: ignore\n",
    "\n",
    "r"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llm",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
